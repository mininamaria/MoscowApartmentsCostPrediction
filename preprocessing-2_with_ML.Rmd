---
title: "R Notebook"
output: html_notebook
---
```{r}
df <- read.csv("~/moscow_flats_dataset_eng.csv", header=TRUE, stringsAsFactors=FALSE)
library(dplyr)
library(tidyr)
library(ggplot2)
library(patchwork)
library(gridExtra)
```
Чистим данные

```{r}
colSums(is.na(df))
```
Мдааа, очень много прпущенных - мб попробовать не дропнуть значения, а заменить на среднее с такими же параметрами? Только вот там, где нет цены - их всё таки дропнуть.

```{r}
df  = df %>% drop_na(price)
colSums(is.na(df))
```
Окей, остальное тоже почистилось немного - это хорошо

```{r}
# Replace NA with mean for specific columns
df$min_to_metro[is.na(df$min_to_metro)] <- mean(df$min_to_metro, na.rm = TRUE)
df$total_area[is.na(df$total_area)] <- mean(df$total_area, na.rm = TRUE)
df$living_area[is.na(df$living_area)] <- mean(df$living_area, na.rm = TRUE)
df$floor[is.na(df$floor)] <- round(mean(df$floor, na.rm = TRUE))
df$number_of_floors[is.na(df$number_of_floors)] <- round(mean(df$number_of_floors, na.rm = TRUE))
df$construction_year[is.na(df$construction_year)] <- round(mean(df$construction_year, na.rm = TRUE))
df$is_new[is.na(df$is_new)] <- round(mean(df$is_new, na.rm = TRUE))
df$is_apartments[is.na(df$is_apartments)] <- round(mean(df$is_apartments, na.rm = TRUE))
df$ceiling_height[is.na(df$ceiling_height)] <- round(mean(df$ceiling_height, na.rm = TRUE), 2)

# Add more columns as needed
colSums(is.na(df))
```
let's round

```{r}
df$min_to_metro = round(df$min_to_metro)
df$living_area = round(df$living_area, 2)
```

```{r}
#df = df %>% distinct() %>% select(-link) %>% mutate(price = price/1000)
```

```{r}
library(skimr)
skim(df)

```
```{r}
df = df %>% mutate(living_area_ratio = living_area / total_area) %>%
  mutate(building_age = 2025 - construction_year) %>% 
  mutate( floor_ratio = floor / number_of_floors)
```
EDA
```{r}
# Numerical columns
numerical_cols <- c('price', 'min_to_metro', 'total_area', 'living_area', 'floor',  'number_of_floors', 'construction_year', 'ceiling_height', 'number_of_rooms', 'living_area_ratio', 'building_age', 'floor_ratio')

# Categorical columns
categorical_cols <- c('region_of_moscow', 'is_new', 'is_apartments')

# Before outlier treatment visualizations

# 1. Histograms for numerical variables (will appear one by one)
for (col in numerical_cols) {
  print(
    ggplot(df, aes_string(x = col)) +
      geom_histogram() +
      ggtitle(paste('Distribution of', col, '(before cleaning)')) +
      theme_minimal()
  )
}

# 2. Boxplots for categorical variables (will appear one by one)
for (col in categorical_cols) {
  print(
    ggplot(df, aes_string(x = col, y = 'price')) +
      geom_boxplot(fill = "lightgreen") +
      ggtitle(paste('Price vs.', col, '(before cleaning)')) +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
  )
}

# 3. Scatter plot
print(
  ggplot(df, aes(x = total_area, y = price)) +
    geom_point(alpha = 0.5, color = "blue") +
    ggtitle('Total Area vs. Price (before cleaning)') +
    theme_minimal()
)

# 4. Correlation matrix
cor_matrix <- df %>% 
  select(all_of(numerical_cols)) %>% 
  cor(use = "complete.obs")

library(reshape2)
melted_cor <- melt(cor_matrix)

print(
  ggplot(melted_cor, aes(x = Var1, y = Var2, fill = value)) +
    geom_tile() +
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
    geom_text(aes(label = round(value, 2)), color = "black", size = 3) +
    ggtitle("Correlation Matrix (before cleaning)") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
)
```
```{r}
# Outlier treatment
for (col in numerical_cols) {
  Q1 <- quantile(df[[col]], 0.25, na.rm = TRUE)
  Q3 <- quantile(df[[col]], 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 3.5 * IQR
  upper_bound <- Q3 + 3.5 * IQR
  
  cat(paste("Number of outliers in", col, "before treatment:", 
            sum(df[[col]] < lower_bound | df[[col]] > upper_bound, na.rm = TRUE), "\n"))
  
  df <- df %>% filter(.data[[col]] >= lower_bound & .data[[col]] <= upper_bound | is.na(.data[[col]]))
  
  cat(paste("Number of outliers in", col, "after treatment:", 
            sum(df[[col]] < lower_bound | df[[col]] > upper_bound, na.rm = TRUE), "\n\n"))
}


```

```{r}
# After outlier treatment visualizations

# 1. Histograms for numerical variables (cleaned)
for (col in numerical_cols) {
  print(
    ggplot(df, aes_string(x = col)) +
      geom_histogram() +
      geom_density(color = "red") +
      ggtitle(paste('Distribution of', col, '(after cleaning)')) +
      theme_minimal()
  )
}

# 2. Boxplots for categorical variables (cleaned)
for (col in categorical_cols) {
  print(
    ggplot(df, aes_string(x = col, y = 'price')) +
      geom_boxplot(fill = "lightgreen") +
      ggtitle(paste('Price vs.', col, '(after cleaning)')) +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
  )
}

# 3. Scatter plot (cleaned)
print(
  ggplot(df, aes(x = total_area, y = price)) +
    geom_point(alpha = 0.5, color = "blue") +
    ggtitle('Total Area vs. Price (after cleaning)') +
    theme_minimal()
)

# 4. Correlation matrix (cleaned)
cor_matrix_clean <- df %>% 
  select(all_of(numerical_cols)) %>% 
  cor(use = "complete.obs")

melted_cor_clean <- melt(cor_matrix_clean)

print(
  ggplot(melted_cor_clean, aes(x = Var1, y = Var2, fill = value)) +
    geom_tile() +
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
    geom_text(aes(label = round(value, 2)), color = "black", size = 3) +
    ggtitle("Correlation Matrix (after cleaning)") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
)
```

```{r}
# Загрузка библиотек
library(dplyr)
library(caret)
library(randomForest)
library(Metrics)

set.seed(123)

# Очистка на случай конфликта
if (exists("df_ml")) rm(df_ml)
if (exists("R2")) rm(R2)

# Создание копии исходного датафрейма
df_ml <- df

# Удаление лишних и мультиколлинеарных признаков
df_ml <- df_ml[, !(names(df_ml) %in% c("link", "total_area", "living_area_ratio", "floor_ratio"))]

# Преобразование категориальных переменных
df_ml$region_of_moscow <- as.factor(df_ml$region_of_moscow)

# Деление на train и test
train_index <- createDataPartition(df_ml$price, p = 0.8, list = FALSE)
train <- df_ml[train_index, ]
test <- df_ml[-train_index, ]

# Обучение моделей

# Линейная регрессия
model_lm <- train(price ~ ., data = train, method = "lm")

# Случайный лес с меньшим числом деревьев
model_rf <- train(price ~ ., data = train, method = "rf", ntree = 50)

# Предсказания и метрики

## Линейная регрессия
cat("\n--- Linear Regression ---\n")
start_lm <- Sys.time()
pred_lm <- predict(model_lm, test)
rmse_lm <- rmse(test$price, pred_lm)
mae_lm <- mae(test$price, pred_lm)
r2_lm <- caret::R2(pred_lm, test$price)
end_lm <- Sys.time()
cat("Time:", round(end_lm - start_lm, 2), "seconds\n")

## Случайный лес
cat("\n--- Random Forest ---\n")
start_rf <- Sys.time()
pred_rf <- predict(model_rf, test)
rmse_rf <- rmse(test$price, pred_rf)
mae_rf <- mae(test$price, pred_rf)
r2_rf <- caret::R2(pred_rf, test$price)
end_rf <- Sys.time()
cat("Time:", round(end_rf - start_rf, 2), "seconds\n")

# Результаты
results <- data.frame(
  Model = c("Linear Regression", "Random Forest"),
  RMSE = c(rmse_lm, rmse_rf),
  MAE = c(mae_lm, mae_rf),
  R2 = c(r2_lm, r2_rf)
)

cat("\n--- Results Summary ---\n")
print(results)
```


```{r}

library(corrplot)

set.seed(123)

# Создание копии исходного датафрейма
df_ml <- df

# 1. Удаление лишних признаков на основе корреляционного анализа
# Удаляем признаки с высокой корреляцией (|cor| > 0.7)
df_ml <- df_ml[, !(names(df_ml) %in% c(
  "link", 
  "total_area",        # Высокая корреляция с price (0.78) и living_area (0.67)
  "living_area_ratio", # Низкая корреляция с price (-0.36)
  "floor_ratio",       # Низкая корреляция с price (0.05)
  "construction_year"  # Почти полная корреляция с building_age (-1.0)
))]



# 3. Преобразование категориальных переменных
df_ml$region_of_moscow <- as.factor(df_ml$region_of_moscow)

# 4. Нормализация числовых переменных (кроме целевой)
preProc <- preProcess(df_ml[, !names(df_ml) %in% c("price", "region_of_moscow")], 
                      method = c("center", "scale"))
df_ml <- predict(preProc, df_ml)

# 5. Деление на train и test с учетом стратификации
train_index <- createDataPartition(df_ml$price, p = 0.8, list = FALSE)
train <- df_ml[train_index, ]
test <- df_ml[-train_index, ]

# 6. Обучение моделей с настройкой параметров

# Линейная регрессия с перекрестной проверкой
ctrl <- trainControl(method = "cv", number = 5)
model_lm <- train(price ~ ., 
                  data = train, 
                  method = "lm",
                  trControl = ctrl)

# Случайный лес с настройкой параметров
tuneGrid <- expand.grid(.mtry = c(2, 4, 6))
model_rf <- train(price ~ ., 
                 data = train, 
                 method = "rf",
                 ntree = 100,
                 tuneGrid = tuneGrid,
                 trControl = ctrl,
                 importance = TRUE)

# 7. Анализ важности переменных
varImpPlot(model_rf$finalModel)

# 8. Предсказания и метрики

## Линейная регрессия
cat("\n--- Linear Regression ---\n")
pred_lm <- predict(model_lm, test)
rmse_lm <- rmse(test$price, pred_lm)
mae_lm <- mae(test$price, pred_lm)
r2_lm <- caret::R2(pred_lm, test$price)

## Случайный лес
cat("\n--- Random Forest ---\n")
pred_rf <- predict(model_rf, test)
rmse_rf <- rmse(test$price, pred_rf)
mae_rf <- mae(test$price, pred_rf)
r2_rf <- caret::R2(pred_rf, test$price)

# 9. Результаты
results <- data.frame(
  Model = c("Linear Regression", "Random Forest"),
  RMSE = c(rmse_lm, rmse_rf),
  MAE = c(mae_lm, mae_rf),
  R2 = c(r2_lm, r2_rf)
)

cat("\n--- Results Summary ---\n")
print(results)

# 10. Дополнительный анализ остатков
par(mfrow = c(1, 2))
plot(pred_lm, test$price - pred_lm, main = "Residuals: Linear Regression")
abline(h = 0, col = "red")
plot(pred_rf, test$price - pred_rf, main = "Residuals: Random Forest")
abline(h = 0, col = "red")
```

Градиентный бустинг (лучшая модель)

```{r}
library(xgboost)
library(caret)
library(Metrics)

# Подготовка данных
df_ml <- df[, !(names(df) %in% c("link", "total_area", "living_area_ratio", "floor_ratio", "construction_year"))]
df_ml$region_of_moscow <- as.factor(df_ml$region_of_moscow)

# Разделение на train/test
set.seed(123)
train_index <- createDataPartition(df_ml$price, p = 0.8, list = FALSE)
train <- df_ml[train_index, ]
test <- df_ml[-train_index, ]

# Создание dummy-переменных и сохранение преобразователя
dummy <- dummyVars(~ ., data = train[, -which(names(train) == "price")])
train_xgb <- predict(dummy, train)
test_xgb <- predict(dummy, test)

# Сохранение объектов для Shiny
saveRDS(dummy, "preprocessor.rds")

# Конфигурация кросс-валидации
ctrl <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE,
  savePredictions = "final"
)

# Сетка параметров
tune_grid <- expand.grid(
  nrounds = c(100, 200),
  eta = c(0.01, 0.05, 0.1),
  max_depth = c(4, 6, 8),
  gamma = 0,
  colsample_bytree = 0.8,
  min_child_weight = 1,
  subsample = 0.8
)

# Обучение модели
set.seed(123)
xgb_model <- train(
  x = train_xgb,
  y = train$price,
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = tune_grid,
  metric = "RMSE"
)

# Сохранение лучшей модели
saveRDS(xgb_model, "best_xgb_model.rds")

# Оценка на тестовых данных
predictions <- predict(xgb_model, test_xgb)

cat("\n=== Final Model Performance ===\n")
cat("RMSE:", rmse(test$price, predictions), "\n")
cat("MAE:", mae(test$price, predictions), "\n")
cat("R²:", cor(test$price, predictions)^2, "\n")

# Важность признаков
importance <- varImp(xgb_model)
plot(importance, top = 10)
```



